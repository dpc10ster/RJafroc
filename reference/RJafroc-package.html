<!DOCTYPE html>
<!-- Generated by pkgdown: do not edit by hand --><html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"><meta charset="utf-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0"><title>Artificial Intelligence Systems and Observer Performance — RJafroc-package • RJafroc</title><!-- jquery --><script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.4.1/jquery.min.js" integrity="sha256-CSXorXvZcTkaix6Yvo6HppcZGetbYMGWSFlBw8HfCJo=" crossorigin="anonymous"></script><!-- Bootstrap --><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/twitter-bootstrap/3.4.1/css/bootstrap.min.css" integrity="sha256-bZLfwXAP04zRMK2BjiO8iu9pf4FbLqX6zitd+tIvLhE=" crossorigin="anonymous"><script src="https://cdnjs.cloudflare.com/ajax/libs/twitter-bootstrap/3.4.1/js/bootstrap.min.js" integrity="sha256-nuL8/2cJ5NDSSwnKD8VqreErSWHtnEP9E7AySL+1ev4=" crossorigin="anonymous"></script><!-- bootstrap-toc --><link rel="stylesheet" href="../bootstrap-toc.css"><script src="../bootstrap-toc.js"></script><!-- Font Awesome icons --><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.12.1/css/all.min.css" integrity="sha256-mmgLkCYLUQbXn0B1SRqzHar6dCnv9oZFPEC1g1cwlkk=" crossorigin="anonymous"><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.12.1/css/v4-shims.min.css" integrity="sha256-wZjR52fzng1pJHwx4aV2AO3yyTOXrcDW7jBpJtTwVxw=" crossorigin="anonymous"><!-- clipboard.js --><script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.6/clipboard.min.js" integrity="sha256-inc5kl9MA1hkeYUt+EC3BhlIgyp/2jDIyBLS6k3UxPI=" crossorigin="anonymous"></script><!-- headroom.js --><script src="https://cdnjs.cloudflare.com/ajax/libs/headroom/0.11.0/headroom.min.js" integrity="sha256-AsUX4SJE1+yuDu5+mAVzJbuYNPHj/WroHuZ8Ir/CkE0=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/headroom/0.11.0/jQuery.headroom.min.js" integrity="sha256-ZX/yNShbjqsohH1k95liqY9Gd8uOiE1S4vZc+9KQ1K4=" crossorigin="anonymous"></script><!-- pkgdown --><link href="../pkgdown.css" rel="stylesheet"><script src="../pkgdown.js"></script><meta property="og:title" content="Artificial Intelligence Systems and Observer Performance — RJafroc-package"><meta property="og:description" content="RJafroc analyzes the performance of artificial intelligence (AI) systems/algorithms characterized 
    by a search-and-report strategy. Historically observer performance has dealt with measuring radiologists' 
    performances in search tasks, e.g., searching for lesions in medical images and reporting them, but the implicit 
    location information has been ignored. The methods here apply to any task involving searching for 
    and reporting arbitrary targets in images. The implemented methods apply 
    to analyzing the absolute and relative performances of AI systems, comparing AI performance to a group of human 
    readers or optimizing the reporting threshold of an AI system. In addition to performing historical receiver operating 
    characteristic (ROC) analysis (localization information ignored), the software also performs free-response receiver operating
    characteristic (FROC) analysis, where the implicit lesion localization information is used. A book describing the 
    underlying methodology and which uses the software has been 
    published: Chakraborty DP: Observer Performance Methods for Diagnostic Imaging - Foundations, Modeling, and Applications 
    with R-Based Examples, Taylor-Francis LLC; 2017. Online updates to this book, which use the software, are at 
    https://dpc10ster.github.io/RJafrocQuickStart/, https://dpc10ster.github.io/RJafrocRocBook/ and at 
    https://dpc10ster.github.io/RJafrocFrocBook/. Supported data collection paradigms are the ROC, FROC and the location ROC (LROC). 
    ROC data consists of single ratings per images, where a rating is the perceived confidence level that the image is that of a 
    diseased patient. An ROC curve is a plot of true positive fraction vs. false 
    positive fraction. FROC data consists of a variable number (zero or more) of mark-rating pairs per image, where a mark is the 
    location of a reported suspicious region and the rating is the confidence level that it is a real lesion. LROC data 
    consists of a rating and a location of the most suspicious region, for every image. Four models of observer performance, 
    and curve-fitting software, are implemented: the binormal model (BM), the contaminated binormal model (CBM), the 
    correlated contaminated binormal model (CORCBM), and the radiological search model (RSM). Unlike the binormal model, CBM, 
    CORCBM and RSM predict &quot;proper&quot; ROC curves that do not inappropriately cross the chance diagonal. Additionally, RSM 
    parameters are related to search performance (not measured in conventional ROC analysis) and classification performance. 
    Search performance refers to finding lesions, i.e., true positives, while simultaneously not finding false positive 
    locations. Classification performance measures the ability to distinguish between true and false positive locations. Knowing 
    these separate performances allows principled optimization of reader or AI system performance. This package supersedes Windows 
    JAFROC (jackknife alternative FROC) software V4.2.1, https://github.com/dpc10ster/WindowsJafroc. Package functions 
    are organized as follows. Data file related function names are preceded by Df, curve fitting functions by Fit, included 
    data sets by dataset, plotting functions by Plot, significance testing functions by St, sample size related functions
    by Ss, data simulation functions by Simulate and utility functions by Util. Implemented are figures of merit (FOMs) 
    for quantifying performance, functions for visualizing empirical operating characteristics: e.g., ROC, FROC, alternative FROC 
    (AFROC) and weighted AFROC (wAFROC) curves. For fully crossed study designs significance testing of reader-averaged FOM 
    differences between modalities is implemented via both Dorfman-Berbaum-Metz and the Obuchowski-Rockette methods. Also 
    implemented are single treatment analyses, allowing comparison of performance of a group of radiologists to a specified 
    value, or comparison of AI to a group of radiologists/algorithms interpreting the same cases. Crossed-modality analysis is implemented 
    wherein there are two crossed treatment factors and the aim is to determined performance in each treatment factor averaged 
    over all levels of the second factor. Sample size estimation tools are provided for ROC and FROC studies; these use estimates 
    of the relevant variances from a pilot study to predict required numbers of readers and cases in a pivotal study to achieve the
    desired power. Utility and data file manipulation functions allow data to be read in any of the currently used input
    formats, including Excel, and the results of the analysis can be viewed in text or Excel output files. The methods are
    illustrated with several included datasets from the author's collaborations. This update includes improvements to the code, 
    some as a result of user-reported bugs and new feature requests, and others discovered during ongoing testing and code simplification. 
    All changes are noted in NEWS.md."><!-- mathjax --><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js" integrity="sha256-nvJJv9wWKEm88qvoQl9ekL2J+k/RWIsaSScxxlsrv8k=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/config/TeX-AMS-MML_HTMLorMML.js" integrity="sha256-84DKXVJXs0/F8OTMzX4UR909+jtl4G7SPypPavF+GfA=" crossorigin="anonymous"></script><!--[if lt IE 9]>
<script src="https://oss.maxcdn.com/html5shiv/3.7.3/html5shiv.min.js"></script>
<script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
<![endif]--></head><body data-spy="scroll" data-target="#toc">
    

    <div class="container template-reference-topic">
      <header><div class="navbar navbar-default navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false">
        <span class="sr-only">Toggle navigation</span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <span class="navbar-brand">
        <a class="navbar-link" href="../index.html">RJafroc</a>
        <span class="version label label-default" data-toggle="tooltip" data-placement="bottom" title="">2.1.0.9000</span>
      </span>
    </div>

    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav"><li>
  <a href="../reference/index.html">Reference</a>
</li>
<li>
  <a href="../news/index.html">Changelog</a>
</li>
      </ul><ul class="nav navbar-nav navbar-right"></ul></div><!--/.nav-collapse -->
  </div><!--/.container -->
</div><!--/.navbar -->

      

      </header><div class="row">
  <div class="col-md-9 contents">
    <div class="page-header">
    <h1>Artificial Intelligence Systems and Observer Performance</h1>
    
    <div class="hidden name"><code>RJafroc-package.Rd</code></div>
    </div>

    <div class="ref-description">
    <p><code>RJafroc</code> analyzes the performance of artificial intelligence (AI) systems/algorithms characterized 
    by a <em>search-and-report</em> strategy. Historically observer performance has dealt with measuring radiologists' 
    performances in search tasks, e.g., searching for lesions in medical images and reporting them, but the implicit 
    location information has been ignored. The methods here apply to <em>any</em> task involving searching for 
    and reporting arbitrary targets in images. The implemented methods apply 
    to analyzing the absolute and relative performances of AI systems, comparing AI performance to a group of human 
    readers or optimizing the reporting threshold of an AI system. In addition to performing historical receiver operating 
    characteristic (ROC) analysis (localization information ignored), the software also performs free-response receiver operating
    characteristic (FROC) analysis, where the implicit lesion localization information is used. A book describing the 
    underlying methodology and which uses the software has been 
    published: <strong>Chakraborty DP: Observer Performance Methods for Diagnostic Imaging - Foundations, Modeling, and Applications 
    with R-Based Examples, Taylor-Francis LLC; 2017</strong>. Online updates to this book, which use the software, are at 
    <a href="https://dpc10ster.github.io/RJafrocQuickStart/" class="external-link">https://dpc10ster.github.io/RJafrocQuickStart/</a>, <a href="https://dpc10ster.github.io/RJafrocRocBook/" class="external-link">https://dpc10ster.github.io/RJafrocRocBook/</a> and at 
    <a href="https://dpc10ster.github.io/RJafrocFrocBook/" class="external-link">https://dpc10ster.github.io/RJafrocFrocBook/</a>. Supported data collection paradigms are the ROC, FROC and the location ROC (LROC). 
    ROC data consists of single ratings per images, where a rating is the perceived confidence level that the image is that of a 
    diseased patient. An ROC curve is a plot of true positive fraction vs. false 
    positive fraction. FROC data consists of a variable number (zero or more) of mark-rating pairs per image, where a mark is the 
    location of a reported suspicious region and the rating is the confidence level that it is a real lesion. LROC data 
    consists of a rating and a location of the most suspicious region, for every image. Four models of observer performance, 
    and curve-fitting software, are implemented: the binormal model (BM), the contaminated binormal model (CBM), the 
    correlated contaminated binormal model (CORCBM), and the radiological search model (RSM). Unlike the binormal model, CBM, 
    CORCBM and RSM predict "proper" ROC curves that do not inappropriately cross the chance diagonal. Additionally, RSM 
    parameters are related to search performance (not measured in conventional ROC analysis) and classification performance. 
    Search performance refers to finding lesions, i.e., true positives, while simultaneously not finding false positive 
    locations. Classification performance measures the ability to distinguish between true and false positive locations. Knowing 
    these separate performances allows principled optimization of reader or AI system performance. This package supersedes Windows 
    <strong>JAFROC</strong> (jackknife alternative FROC) software V4.2.1, <a href="https://github.com/dpc10ster/WindowsJafroc" class="external-link">https://github.com/dpc10ster/WindowsJafroc</a>. Package functions 
    are organized as follows. Data file related function names are preceded by <em>Df</em>, curve fitting functions by <em>Fit</em>, included 
    data sets by <em>dataset</em>, plotting functions by <em>Plot</em>, significance testing functions by <em>St</em>, sample size related functions
    by <em>Ss</em>, data simulation functions by <em>Simulate</em> and utility functions by <em>Util</em>. Implemented are figures of merit (FOMs) 
    for quantifying performance, functions for visualizing empirical operating characteristics: e.g., ROC, FROC, alternative FROC 
    (AFROC) and weighted AFROC (wAFROC) curves. For fully crossed study designs significance testing of reader-averaged FOM 
    differences between modalities is implemented via both Dorfman-Berbaum-Metz and the Obuchowski-Rockette methods. Also 
    implemented are single treatment analyses, allowing comparison of performance of a group of radiologists to a specified 
    value, or comparison of AI to a group of radiologists/algorithms interpreting the same cases. Crossed-modality analysis is implemented 
    wherein there are two crossed treatment factors and the aim is to determined performance in each treatment factor averaged 
    over all levels of the second factor. Sample size estimation tools are provided for ROC and FROC studies; these use estimates 
    of the relevant variances from a pilot study to predict required numbers of readers and cases in a pivotal study to achieve the
    desired power. Utility and data file manipulation functions allow data to be read in any of the currently used input
    formats, including Excel, and the results of the analysis can be viewed in text or Excel output files. The methods are
    illustrated with several included datasets from the author's collaborations. This update includes improvements to the code, 
    some as a result of user-reported bugs and new feature requests, and others discovered during ongoing testing and code simplification. 
    All changes are noted in <strong>NEWS.md</strong>.</p>
    </div>


    <div id="details">
    <h2>Details</h2>
    <p></p><table class="table table"><tr><td>Package:</td><td>RJafroc</td></tr><tr><td>Type:</td><td>Package</td></tr><tr><td>Version:</td><td>2.1.0.9000</td></tr><tr><td>Date:</td><td>2022-07-22</td></tr><tr><td>License:</td><td>GPL-3</td></tr><tr><td>URL:</td><td><a href="https://dpc10ster.github.io/RJafroc/" class="external-link">https://dpc10ster.github.io/RJafroc/</a></td></tr></table></div>
    <div id="definitions-and-abbreviations">
    <h2>Definitions and abbreviations</h2>
    <p></p><ul><li><p><em>a</em>: The separation or "a" parameter of the binormal model</p></li>
<li><p>AFROC curve: plot of LLF (ordinate) vs. FPF, where FPF is inferred using 
       highest rating of NL marks on <strong>non-diseased cases</strong></p></li>
<li><p>AFROC: alternative FROC, see Chakraborty 1989</p></li>
<li><p>AFROC1 curve: plot of LLF (ordinate) vs. FPF1, where FPF1 is inferred using 
       highest rating of NL marks on <strong>ALL cases</strong></p></li>
<li><p>\(alpha\): The significance level \(\alpha\) of the test of the null 
       hypothesis of no treatment effect</p></li>
<li><p>AUC: area under curve; e.g., ROC-AUC = area under ROC curve, an 
       example of a FOM</p></li>
<li><p><em>b</em>: The width or "b" parameter of the conventional binormal 
       model</p></li>
<li><p>Binormal model: two unequal variance normal distributions, one at zero 
       and one at \(mu\), for modeling ROC ratings, \(sigma\) is the 
       std. dev. ratio of diseased to non-diseased distributions</p></li>
<li><p>CAD: computer aided detection algorithm</p></li>
<li><p>CBM: contaminated binormal model (CBM): two equal variance normal 
       distributions for modeling ROC ratings, the diseased distribution is 
       bimodal, with a peak at zero and one at \(\mu\), the integrated fraction 
       at \(\mu\) is \(\alpha\) (not to be confused with \(\alpha\) of NH testing)</p></li>
<li><p>CI: The (1-\(\alpha\)) confidence interval for the stated statistic</p></li>
<li><p>Crossed modality: a dataset containing two modality (treatment) factors, with 
       the levels of the two factors crossed, see paper by Thompson et al</p></li>
<li><p>DBM: Dorfman-Berbaum-Metz, a significance testing method for 
       detecting a treatment effect in MRMC studies</p></li>
<li><p>DBMH: Hillis' modification of the DBM method</p></li>
<li><p>ddf: Denominator degrees of freedom of appropriate \(F\)-test; 
       the corresponding ndf is <code>I</code> - 1</p></li>
<li><p>Empirical AUC: trapezoidal area under curve, same as the Wilcoxon 
       statistic for ROC paradigm</p></li>
<li><p>FN: false negative, a diseased case classified as non-diseased</p></li>
<li><p>FOM: figure of merit, a quantitative measure of performance, 
       performance metric</p></li>
<li><p>FP: false positive, a non-diseased case classified as diseased</p></li>
<li><p>FPF: number of FPs divided by number of non-diseased cases</p></li>
<li><p>FROC curve: plot of LLF (ordinate) vs. NLF</p></li>
<li><p>FROC: free-response ROC (a data collection paradigm where each image 
       yields a random number, 0, 1, 2,..., of mark-rating pairs)</p></li>
<li><p>FRRC: Analysis that treats readers as fixed and cases as random factors</p></li>
<li><p>I: total number of modalities, indexed by \(i\)</p></li>
<li><p>image/case: used interchangeably; a case can consist of several images 
       of the same patient in the same treatment</p></li>
<li><p>iMRMC: A text file format used for ROC data by FDA/CDRH researchers</p></li>
<li><p>individual: A single-treatment single-reader dataset.</p></li>
<li><p>Intrinsic: Used in connection with RSM; a parameter that is independent of 
       the RSM \(\mu\) parameter, but whose meaning may not be as transparent as 
       the corresponding physical parameter</p></li>
<li><p>J: number of readers, indexed by <code>j</code></p></li>
<li><p>JAFROC file format: A .xlsx format file, applicable to ROC, ROI, FROC and 
       LROC paradigms</p></li>
<li><p>JAFROC: jackknife AFROC: Windows software for analyzing observer performance 
       data: no longer updated, replaced by current package; the name is a misnomer 
       as the jackknife is used only for significance testing; alternatively, 
       the bootstrap could be used; what distinguishes FROC from ROC analysis is the 
       use of the AFROC-AUC as the FOM. With this change, the DBM or the OR method 
       can be used for significance testing</p></li>
<li><p><code>K</code>: total number of cases, <code>K</code> = <code>K1</code> + <code>K2</code>, indexed by \(k\)</p></li>
<li><p><code>K1</code>: total number of non-diseased cases, indexed by \(k1\)</p></li>
<li><p><code>K2</code>: total number of diseased cases, indexed by \(k2\)</p></li>
<li><p>LL: lesion localization i.e., a mark that correctly locates an existing 
       localized lesion; TP is a special case, when the proximity criterion is 
      lax  (i.e., "acceptance radius" is large)</p></li>
<li><p>LLF: number of LLs divided by the total number of lesions</p></li>
<li><p>LROC: location receiver operating characteristic, a data collection paradigm 
       where each image yields a single rating and one location</p></li>
<li><p>lrc/MRMC: A text file format used for ROC data by University of Iowa 
       researchers</p></li>
<li><p>mark: the location of a suspected diseased region</p></li>
<li><p>maxLL: maximum number of lesions per case in dataset</p></li>
<li><p>maxNL: maximum number of NL marks per case in dataset</p></li>
<li><p>MRMC: multiple reader multiple case (each reader interprets each case in 
       each treatment, i.e. fully crossed study design)</p></li>
<li><p>ndf: Numerator degrees of freedom of appropriate \(F\)-test, usually 
       number of treatments minus one</p></li>
<li><p>NH: The null hypothesis that all treatment effects are zero; rejected 
       if the \(p\)-value is smaller than \(\alpha\)</p></li>
<li><p>NL: non-lesion localization, of which FP is a special case, i.e., a 
       mark that does not correctly locate any existing localized lesion(s)</p></li>
<li><p>NLF: number of NLs divided by the total number of cases</p></li>
<li><p>Operating characteristic: A plot of normalized correct decisions on 
       diseased cases along ordinate vs. normalized incorrect decisions on 
       non-diseased cases</p></li>
<li><p>Operating point: A point on an operating characteristic, e.g., 
       (FPF, TPF) represents an operating point on an ROC</p></li>
<li><p>OR: Obuchowski-Rockette, a significance testing method for detecting 
       a treatment effect in MRMC studies</p></li>
<li><p>ORH: Hillis' modification of the OR method</p></li>
<li><p>Physical parameter: Used in connection with RSM; a parameter whose 
       meaning is more transparent than the corresponding intrinsic parameter, 
       but which depends on the RSM \(\mu\) parameter</p></li>
<li><p>Proximity criterion / acceptance radius: Used in connection with FROC 
       (or LROC data); the "nearness" criterion is used to determine if a mark 
       is close enough to a lesion to be counted as a LL (or correct localization); 
       otherwise it is counted as a NL (or incorrect localization)</p></li>
<li><p>p-value: the probability, under the null hypothesis, that the observed 
       treatment effects, or larger, could occur by chance</p></li>
<li><p>Proper: a proper fit does not inappropriately fall below the chance 
       diagonal, does not display a "hook" near the upper right corner</p></li>
<li><p>PROPROC: Metz's binormal model based fitting of proper ROC curves</p></li>
<li><p>RSM, Radiological Search Model: two unit variance normal distributions 
       for modeling NL and LL ratings; four parameters, \(\mu\), \(\nu\)',
       \(\lambda\)' and \(\zeta\)1</p></li>
<li><p>Rating: Confidence level assigned to a case; higher values indicate greater 
       confidence in presence of disease; <code>-Inf</code> is allowed but <code>NA</code> is 
       not allowed</p></li>
<li><p>Reader/observer/radiologist/CAD: used interchangeably</p></li>
<li><p>RJafroc: the current software</p></li>
<li><p>ROC: receiver operating characteristic, a data collection paradigm where 
       each image yields a single rating and location information is ignored</p></li>
<li><p>ROC curve: plot of TPF (ordinate) vs. FPF, as threshold is varied; 
       an example of an operating characteristic</p></li>
<li><p>ROCFIT: Metz software for binormal model based fitting of ROC data</p></li>
<li><p>ROI: region-of-interest (each case is divided into a number of ROIs and 
       the reader assigns an ROC rating to each ROI)</p></li>
<li><p>FRRC: Analysis that treats readers as fixed and cases as random factors</p></li>
<li><p>RRFC: Analysis that treats readers as random and cases as fixed factors</p></li>
<li><p>RRRC: Analysis that treats both readers and cases as random factors</p></li>
<li><p>RSCORE-II: original software for binormal model based fitting of ROC data</p></li>
<li><p>RSM: Radiological search model, also method for fitting a proper ROC 
       curve to ROC data</p></li>
<li><p>RSM-\(\zeta\)1: Lowest reporting threshold, determines if suspicious 
       region is actually marked</p></li>
<li><p>RSM-\(\lambda\): Intrinsic parameter of RSM corresponding to 
       \(\lambda\)', independent of \(\mu\)</p></li>
<li><p>RSM-\(\lambda\)': Physical Poisson parameter of RSM, average number of 
       latent NLs per case; depends on \(\mu\)</p></li>
<li><p>RSM-\(\mu\): separation of the unit variance distributions of RSM</p></li>
<li><p>RSM-\(\nu\): Intrinsic parameter of RSM, corresponding to \(\nu\)', 
       independent of \(\mu\)</p></li>
<li><p>RSM-\(\nu\)': binomial parameter of RSM, probability that lesion is found</p></li>
<li><p>SE: sensitivity, same as \(TPF\)</p></li>
<li><p>Significance testing: determining the p-value of a statistical test</p></li>
<li><p>SP: specificity, same as \(1-FPF\)</p></li>
<li><p>Threshold: Reporting criteria: if confidence exceeds a threshold value, 
       report case as diseased, otherwise report non-diseased</p></li>
<li><p>TN: true negative, a non-diseased case classified as non-diseased</p></li>
<li><p>TP: true positive, a diseased case classified as diseased</p></li>
<li><p>TPF: number of TPs divided by number of diseased cases</p></li>
<li><p>Treatment/modality: used interchangeably, for example, computed tomography 
       (CT) images vs. magnetic resonance imaging (MRI) images</p></li>
<li><p>wAFROC curve: plot of weighted LLF (ordinate) vs. FPF, where FPF is 
       inferred using highest rating of NL marks on <strong>non-diseased cases ONLY</strong></p></li>
<li><p>wAFROC1 curve: plot of weighted LLF (ordinate) vs. FPF1, where FPF1 is 
       inferred using highest rating of NL marks on <strong>ALL cases</strong></p></li>
<li><p>wAFROC1 FOM: weighted trapezoidal area under AFROC1 curve: only use 
       if there are zero non-diseased cases is always number of treatments minus one</p></li>
</ul></div>
    <div id="dataset">
    <h2>Dataset</h2>
    <p>The <code>dataset</code> object has 3 <code>list</code> elements: <code>$ratings</code>, <code>$lesions</code> 
     and <code>$descriptions</code>, where:</p>     
<p></p><ul><li><p><code>dataset$ratings</code>: contains 3 elements as sub-lists: <code>$NL</code>, 
        <code>$LL</code> and <code>$LL_IL</code>; these describe the structure of the ratings;</p></li>
<li><p><code>dataset$lesions</code>: contains 3 elements as sub-lists: <code>$perCase</code>, 
        <code>$IDs</code> and <code>$weights</code>; these describe the structure of the lesions;</p></li>
<li><p><code>dataset$descriptions</code>: contains 7 elements as sub-lists: <code>$fileName</code>, 
        <code>$type</code>, <code>$name</code>, <code>$truthTableStr</code>, <code>$design</code>, <code>$modalityID</code> 
        and <code>$readerID</code>; these describe other characteristics of the dataset as 
        detailed next.</p></li>
</ul><p><strong>Note: <code>-Inf</code> is used to indicate the ratings of unmarked lesions 
        and/or missing values.</strong> As an example of the latter, if the maximum 
        number of NLs in a dataset is 4, but some images have fewer than 4 NL marks, 
        the corresponding "empty" positions would be filled with 
        <code>-Inf</code>s. <strong>Do not use <code>NA</code> to denote a missing rating.</strong></p>     
<p><strong>Note</strong>: <b>"dataset" in this package always represents
    <code>R</code> object(s) with the following structure(s):</b></p>    
<p></p><div class="section">
<h3 id="general-data-structure-e-g-dataset-an-roc-dataset-and-dataset-an-froc-dataset-">General data structure, e.g., <code>dataset02</code>, an ROC dataset, and 
        <code>dataset05</code>, an FROC dataset.<a class="anchor" aria-label="anchor" href="#general-data-structure-e-g-dataset-an-roc-dataset-and-dataset-an-froc-dataset-"></a></h3>
<p></p><ul><li><p><code>ratings$NL</code>: a float array with dimensions
        <code>c(I, J, K, maxNL)</code>, containing the ratings of NL marks. The first 
        <code>K1</code> locations of the third index corresponds to NL marks on non-diseased 
        cases and the remaining locations correspond to NL marks on diseased 
        cases. The 4th dimension allows for multiple NL marks 
        on a case: the first index holds the first NL rating on the image, 
        the second holds the second NL rating on the image, etc. The value of 
        <code>maxNL</code> is determined by the case with the maximum number of lesions 
        per case in the dataset. For <b>FROC</b> datasets missing NL ratings are assigned the 
        <code>-Inf</code> rating. For <b>ROC</b> datasets, FP ratings are assigned 
        to the first <code>K1</code> elements of <code>NL[,,1:K1,1]</code> and the remaining 
        <code>K2</code> elements of <code>NL[,,(K1+1):K,1]</code> are set to <code>-Inf</code>.</p></li>
<li><p><code>ratings$LL</code>: for non-LROC datasets a float array with dimensions   
        <code>c(I, J, K2, maxLL)</code> containing the ratings of LL marks. The value of 
        <code>maxLL</code> is determined by the maximum number of lesions per case in the 
        dataset. Unmarked lesions are assigned the <code>-Inf</code> rating.
        For ROC datasets <code>TP</code> ratings are assigned to <code>LL[,,1:K2,1]</code>.
        For <b>LROC</b> datasets it is a float array with dimensions  
        <code>c(I, J, K2, 1)</code> containing the ratings of correct localizations, 
        otherwise the rating is recorded in the incorrect localization array
        described next.</p></li>
<li><p><code>ratings$LL_IL</code>: for LROC datasets the ratings
        of incorrect localization marks on abnormal cases. It is a float array 
        with dimensions <code>c(I, J, K2, 1)</code>. For non-LROC datasets this array 
        is filled with NAs.</p></li>
<li><p><code>lesions$perCase</code>: an integer array with length <code>K2</code>, 
        the number of lesions on each diseased case. The 
        maximum value of this array equals <code>maxLL</code>. For example,  
        <code>dataset05$lesions$perCase[4</code> is 2, meaning the 
        4th diseased case has two lesions.</p></li>
<li><p><code>lesions$IDs</code>: an integer array with dimensions [<code>K2</code>, <code>maxLL</code>], 
        labeling (or naming) the lesions on the diseased cases. For example,  
        <code>dataset05$lesions$IDs[4,]</code> is <code>c(1,2,-Inf)</code>, meaning the 
        4th diseased case has two lesions, labeled 1 and 2.</p></li>
<li><p><code>lesions$weights</code>: a floating point array with dimensions 
        <code>c(K2, maxLL)</code>, representing the relative importance of detecting 
        each lesion. The weights for an abnormal case must sum to unity. 
        For example, <code>dataset05$lesions$weights[4,]</code> is <code>c(0.5,0.5, -Inf)</code>, 
        corresponding to equal weights (0.5) assigned to of the two lesions in the
        case.</p></li>
<li><p><code>descriptions$fileName</code>: a <code>character</code> variable containing the file
        name of the source data for this dataset. This is generated automatically
        by the <a href="DfReadDataFile.html">DfReadDataFile</a> function used to read the file. For a simulalated
        dataset it is set to "NA" (i.e., a character vector, not the variable <code>NA</code>).</p></li>
<li><p><code>descriptions$type</code>: a <code>character</code> variable describing the data type: 
        "<code>ROC</code>", "<code>LROC</code>", "<code>ROI</code>" or "<code>FROC</code>".</p></li>
<li><p><code>descriptions$name</code>: a <code>character</code> variable containing the name of 
        the dataset: e.g., "dataset02" or "dataset05". This is generated automatically
        by the <a href="DfReadDataFile.html">DfReadDataFile</a> function used to read the file.</p></li>
<li><p><code>descriptions$truthTableStr</code>: a <code>c(I, J, L, maxLL+1)</code> object. For
        normal cases elements <code>c(I, J, L, 1)</code> are filled with 1s if the corresponding 
        interpretations occurred or NAs otherwise. For abnormal cases elements
        <code>c(I, J, L, 2:(maxLL+1))</code> are filled with 1s if the corresponding 
        interpretations occurred or NAs otherwise. This object is necessary for analyzing
        more complex designs, e.g., split-plot, as described next.</p></li>
<li><p><code>descriptions$design</code>: a <code>character</code> variable: "<code>FCTRL</code>", 
        "<code>SPLIT-PLOT-A</code>" or "<code>SPLIT-PLOT-A</code>", corresponding to factorial,
        split-plot-A or split-plot-C designs. The A and C refer to subparts of Table VII
        in a Hillis 2014 publication.</p></li>
<li><p><code>descriptions$modalityID</code>: a <code>character</code> vector of length \(I\), 
        which labels/names the modalities in the dataset. For non-JAFROC data file formats,
        they must be unique integers.</p></li>
<li><p><code>descriptions$readerID</code>: a <code>character</code> vector of length \(J\), 
        which labels/names the readers in the dataset. For non-JAFROC data file formats,
        they must be unique integers.</p></li>
</ul><p></p>
</div>
  
<p></p><div class="section">
<h3 id="roi-data-structure-example-datasetroi">ROI data structure, example <code>datasetROI</code><a class="anchor" aria-label="anchor" href="#roi-data-structure-example-datasetroi"></a></h3>
<p>Only changes from the previously described structure are described below:</p><ul><li><p><code>ratings$NL</code>: a float array with dimensions <code>c(I, J, K, Q)</code> 
        containing the ratings of each of Q quadrants for each non-diseased case.</p></li>
<li><p><code>ratings$LL</code>: a float array with dimensions  
        <code>c(I, J, K2, Q)</code> containing the ratings of quadrants for each 
        diseased case.</p></li>
<li><p><code>lesions$perCase</code>: this contains the locations, on abnormal cases,
    containing at least one lesion.</p></li>
</ul><p></p>
</div>

<p></p><div class="section">
<h3 id="crossed-modality-data-structure-example-datasetcrossedmodality">Crossed modality data structure, example <code>datasetCrossedModality</code><a class="anchor" aria-label="anchor" href="#crossed-modality-data-structure-example-datasetcrossedmodality"></a></h3>
<p>Only changes from the previously described structure are described below:</p><ul><li><p><code>ratings$NL</code>: a float array with dimension <code>c(I1, I2, J, K, maxNL)</code> 
        containing the ratings of NL marks. Note the existence of two modality indices.</p></li>
<li><p><code>LL</code>: a float array with dimension <code>c(I1, I2, J, K2, maxLL)</code>
        containing the ratings of all LL marks. Note the existence of two modality indices.</p></li>
<li><p><code>modalityID1</code>: corresponding to first modality factor.</p></li>
<li><p><code>modalityID2</code>: corresponding to second modality factor.</p></li>
</ul><p></p>
</div>

    </div>
    <div id="df-datafile-related-functions">
    <h2>Df: Datafile Related Functions</h2>
    <p></p><ul><li><p><code><a href="Df2RJafrocDataset.html">Df2RJafrocDataset</a></code>: Convert a ratings array to a 
           dataset object.</p></li>
<li><p><code><a href="DfBinDataset.html">DfBinDataset</a></code>: Return a binned dataset.</p></li>
<li><p><code><a href="DfCreateCorCbmDataset.html">DfCreateCorCbmDataset</a></code>:Create paired dataset for 
           testing <a href="FitCorCbm.html">FitCorCbm</a>.</p></li>
<li><p><code><a href="DfExtractDataset.html">DfExtractDataset</a></code>: Extract a subset of modalities 
           and readers from a dataset.</p></li>
<li><p><code><a href="DfFroc2Roc.html">DfFroc2Roc</a></code>: Convert an FROC dataset to a highest 
           rating inferred ROC dataset.</p></li>
<li><p><code><a href="DfLroc2Roc.html">DfLroc2Roc</a></code>: Convert an LROC dataset to a highest 
           rating inferred ROC dataset.</p></li>
<li><p><code><a href="DfLroc2Froc.html">DfLroc2Froc</a></code>: Simulates an 
           "AUC-equivalent" FROC dataset from a supplied LROC dataset.</p></li>
<li><p><code><a href="DfFroc2Lroc.html">DfFroc2Lroc</a></code>: Simulates an 
           "AUC-equivalent" LROC dataset from a supplied FROC dataset.</p></li>
<li><p><code><a href="DfReadCrossedModalities.html">DfReadCrossedModalities</a></code>: Read a crossed-modalities 
           data file.</p></li>
<li><p><code><a href="DfReadDataFile.html">DfReadDataFile</a></code>: Read a general data file.</p></li>
<li><p><code><a href="DfReadLrocDataFile.html">DfReadLrocDataFile</a></code>: Read a LROC data file.</p></li>
<li><p><code><a href="DfSaveDataFile.html">DfSaveDataFile</a></code>: Save ROC data file in a different format.</p></li>
<li><p><code><a href="DfExtractCorCbmDataset.html">DfExtractCorCbmDataset</a></code>: Extract two arms of a pairing 
           from an MRMC ROC dataset suitable for using <a href="FitCorCbm.html">FitCorCbm</a>.</p></li>
</ul></div>
    <div id="fitting-functions">
    <h2>Fitting Functions</h2>
    <p></p><ul><li><p><code><a href="FitBinormalRoc.html">FitBinormalRoc</a></code>: Fit the binormal model to ROC data 
           (R equivalent of ROCFIT or RSCORE).</p></li>
<li><p><code><a href="FitCbmRoc.html">FitCbmRoc</a></code>: Fit the contaminated binormal model (CBM) 
           to ROC data.</p></li>
<li><p><code><a href="FitRsmRoc.html">FitRsmRoc</a></code>: Fit the radiological search model (RSM) 
           to ROC data.</p></li>
<li><p><code><a href="FitCorCbm.html">FitCorCbm</a></code>: Fit the correlated contaminated binormal model 
             (CORCBM) to paired ROC data.</p></li>
<li><p><code><a href="FitRsmRoc.html">FitRsmRoc</a></code>: Fit the radiological search model (RSM) 
           to ROC data.</p></li>
</ul></div>
    <div id="plotting-functions">
    <h2>Plotting Functions</h2>
    <p></p><ul><li><p><code><a href="PlotBinormalFit.html">PlotBinormalFit</a></code>: Plot binormal-predicted ROC curve with 
           provided BM parameters.</p></li>
<li><p><code><a href="PlotEmpiricalOperatingCharacteristics.html">PlotEmpiricalOperatingCharacteristics</a></code>: Plot empirical 
           operating characteristics for specified dataset.</p></li>
<li><p><code><a href="PlotRsmOperatingCharacteristics.html">PlotRsmOperatingCharacteristics</a></code>: Plot RSM-fitted ROC curves.</p></li>
</ul></div>
    <div id="simulation-functions">
    <h2>Simulation Functions</h2>
    <p></p><ul><li><p><code><a href="SimulateFrocDataset.html">SimulateFrocDataset</a></code>: Simulates an uncorrelated FROC dataset 
           using the RSM.</p></li>
<li><p><code><a href="SimulateRocDataset.html">SimulateRocDataset</a></code>: Simulates an uncorrelated binormal 
           model ROC dataset.</p></li>
<li><p><code><a href="SimulateCorCbmDataset.html">SimulateCorCbmDataset</a></code>: Simulates an uncorrelated binormal 
           model ROC dataset.</p></li>
<li><p><code><a href="SimulateLrocDataset.html">SimulateLrocDataset</a></code>: Simulates an uncorrelated LROC dataset.</p></li>
</ul></div>
    <div id="sample-size-functions">
    <h2>Sample size Functions</h2>
    <p></p><ul><li><p><code><a href="SsPowerGivenJK.html">SsPowerGivenJK</a></code>: Calculate statistical power given 
           numbers of readers J and cases <code>K</code>.</p></li>
<li><p><code><a href="SsPowerTable.html">SsPowerTable</a></code>: Generate a power table.</p></li>
<li><p><code><a href="SsSampleSizeKGivenJ.html">SsSampleSizeKGivenJ</a></code>: Calculate number of cases <code>K</code>, for 
           specified number of readers J, to achieve desired power for an ROC study.</p></li>
</ul></div>
    <div id="significance-testing-functions">
    <h2>Significance Testing Functions</h2>
    <p></p><ul><li><p><code><a href="StSignificanceTesting.html">StSignificanceTesting</a></code>: Perform significance testing, 
           DBM or OR.</p></li>
<li><p><code><a href="StSignificanceTestingCadVsRad.html">StSignificanceTestingCadVsRad</a></code>: Perform 
           significance testing, CAD vs. radiologists.</p></li>
<li><p><code><a href="StSignificanceTestingCrossedModalities.html">StSignificanceTestingCrossedModalities</a></code>: Perform 
           significance testing using crossed modalities analysis.</p></li>
</ul></div>
    <div id="miscellaneous-and-utility-functions">
    <h2>Miscellaneous and Utility Functions</h2>
    <p></p><ul><li><p><code><a href="UtilAucBinormal.html">UtilAucBinormal</a></code>: Binormal model AUC function.</p></li>
<li><p><code><a href="UtilAucCBM.html">UtilAucCBM</a></code>: CBM AUC function.</p></li>
<li><p><code><a href="UtilAucPROPROC.html">UtilAucPROPROC</a></code>: PROPROC AUC function.</p></li>
<li><p><code><a href="UtilAnalyticalAucsRSM.html">UtilAnalyticalAucsRSM</a></code>: RSM ROC/AFROC AUC calculator.</p></li>
<li><p><code><a href="UtilFigureOfMerit.html">UtilFigureOfMerit</a></code>: Calculate empirical figures of merit 
           (FOMs) for specified dataset.</p></li>
<li><p><code><a href="UtilIntrinsic2PhysicalRSM.html">UtilIntrinsic2PhysicalRSM</a></code>: Convert from intrinsic to 
           physical RSM parameters.</p></li>
<li><p><code><a href="UtilLesionWeightsMatrix.html">UtilLesionWeightsMatrix</a></code>: Calculates the lesion weights  matrix.</p></li>
<li><p><code><a href="UtilMeanSquares.html">UtilMeanSquares</a></code>: Calculates the mean squares used in the 
           DBMH and ORH methods.</p></li>
<li><p><code><a href="UtilOutputReport.html">UtilOutputReport</a></code>: Generate a formatted report file.</p></li>
<li><p><code><a href="UtilPhysical2IntrinsicRSM.html">UtilPhysical2IntrinsicRSM</a></code>: Convert from physical to 
           intrinsic RSM parameters.</p></li>
<li><p><code><a href="UtilPseudoValues.html">UtilPseudoValues</a></code>: Return jackknife pseudovalues.</p></li>
<li><p><code><a href="UtilVarComponentsDBM.html">UtilVarComponentsDBM</a></code>: Utility for Dorfman-Berbaum-Metz 
           variance components.</p></li>
<li><p><code><a href="UtilORVarComponentsFactorial.html">UtilORVarComponentsFactorial</a></code>: Utility for Obuchowski-Rockette 
           variance components.</p></li>
</ul></div>
    <div id="author">
    <h2>Author</h2>
    
<ul><li><p>Author: Dev Chakraborty <a href="mailto:dpc10ster@gmail.com">dpc10ster@gmail.com</a>.</p></li>
<li><p>Author: Xuetong Zhai <a href="mailto:xuetong.zhai@gmail.com">xuetong.zhai@gmail.com</a>.</p></li>
<li><p>Contributor: Peter Phillips <a href="mailto:peter.phillips@cumbria.ac.uk">peter.phillips@cumbria.ac.uk</a>.</p></li>
</ul></div>
    <div id="references">
    <h2>References</h2>
    <p><b>Basics of ROC</b></p>
<p>Metz, CE (1978). Basic principles of ROC analysis. In Seminars in nuclear medicine 
     (Vol. 8, pp. 283--298). Elsevier.</p>  
<p>Metz, CE (1986). ROC Methodology in Radiologic Imaging. Investigative Radiology, 
     21(9), 720.</p>  
<p>Metz, CE (1989). Some practical issues of experimental design and data analysis in 
     radiological ROC studies. Investigative Radiology, 24(3), 234.</p>  
<p>Metz, CE (2008). ROC analysis in medical imaging: a tutorial review of the 
     literature. Radiological Physics and Technology, 1(1), 2--12.</p>  
<p>Wagner, R. F., Beiden, S. V, Campbell, G., Metz, CE, &amp; Sacks, W. M. (2002). 
     Assessment of medical imaging and computer-assist systems: lessons from recent experience. Academic Radiology, 9(11), 1264--77.</p>  
<p>Wagner, R. F., Metz, CE, &amp; Campbell, G. (2007). Assessment of medical imaging 
     systems and computer aids: a tutorial review. Academic Radiology, 14(6), 723--48.</p>
<p><b>DBM/OR methods and extensions</b></p>
<p>DORFMAN, D. D., BERBAUM, KS, &amp; Metz, CE (1992). Receiver operating characteristic 
     rating analysis: generalization to the population of readers and patients with 
     the jackknife method. Investigative Radiology, 27(9), 723.</p>  
<p>Obuchowski, NA, &amp; Rockette, HE (1994). HYPOTHESIS TESTING OF DIAGNOSTIC ACCURACY 
     FOR MULTIPLE READERS AND MULTIPLE TESTS: AN ANOVA APPROACH WITH DEPENDENT 
     OBSERVATIONS. Communications in Statistics-Simulation and Computation, 
     24(2), 285--308.</p>
<p>Hillis, SL, Berbaum, KS, &amp; Metz, CE (2008). Recent developments in the 
     Dorfman-Berbaum-Metz procedure for multireader ROC study analysis. 
     Academic Radiology, 15(5), 647--61.</p>  
<p>Hillis, SL, Obuchowski, NA, &amp; Berbaum, KS (2011). Power Estimation for 
     Multireader ROC Methods: An Updated and Unified Approach. Acad Radiol, 18, 129--142.</p>  
<p>Hillis, SL SL (2007). A comparison of denominator degrees of freedom methods for 
     multiple observer ROC analysis. Statistics in Medicine, 26(3), 596--619.</p>
<p><b>FROC paradigm</b></p>
<p>Chakraborty DP. Maximum Likelihood analysis of free-response receiver operating 
     characteristic (FROC) data. Med Phys. 1989;16(4):561--568.</p>
<p>Chakraborty, DP, &amp; Berbaum, KS (2004). Observer studies involving detection and 
     localization: modeling, analysis, and validation. Medical Physics, 31(8), 1--18.</p>  
<p>Chakraborty, DP (2006). A search model and figure of merit for observer data acquired 
     according to the free-response paradigm. Physics in Medicine and Biology, 
     51(14), 3449--62.</p>  
<p>Chakraborty, DP (2006). ROC curves predicted by a model of visual search. Physics in 
     Medicine and Biology, 51(14), 3463--82.</p>  
<p>Chakraborty, DP (2011). New Developments in Observer Performance Methodology in 
     Medical Imaging. Seminars in Nuclear Medicine, 41(6), 401--418.</p>  
<p>Chakraborty, DP (2013). A Brief History of Free-Response Receiver Operating 
     Characteristic Paradigm Data Analysis. Academic Radiology, 20(7), 915--919.</p>  
<p>Chakraborty, DP, &amp; Yoon, H.-J. (2008). Operating characteristics predicted 
     by models for diagnostic tasks involving lesion localization. Medical 
     Physics, 35(2), 435.</p>
<p>Thompson JD, Chakraborty DP, Szczepura <code>K</code>, et al. (2016) Effect of reconstruction methods 
     and x-ray tube current-time product  on nodule detection in an anthropomorphic 
     thorax phantom: a crossed-modality JAFROC observer study. Medical Physics. 
     43(3):1265-1274.</p>  
<p>Zhai X, Chakraborty DP. (2017) A bivariate contaminated binormal model for robust 
     fitting of proper ROC curves to a pair of correlated, possibly degenerate, 
     ROC datasets. Medical Physics. doi: 10.1002/mp.12263:2207--2222.</p>  
<p>Hillis SL, Chakraborty DP, Orton CG. ROC or FROC? It depends on the research 
     question. Medical Physics. 2017.</p>  
<p>Chakraborty DP, Nishikawa RM, Orton CG. Due to potential concerns of bias and 
     conflicts of interest, regulatory bodies should not do evaluation methodology 
     research related to their regulatory missions. Medical Physics. 2017.</p>  
<p>Dobbins III JT, McAdams HP, Sabol JM, Chakraborty DP, et al. (2016) Multi-Institutional 
     Evaluation of Digital Tomosynthesis, Dual-Energy Radiography, and Conventional 
     Chest Radiography for the Detection and Management of Pulmonary Nodules. Radiology. 282(1):236-250.</p>  
<p>Warren LM, Mackenzie A, Cooke J, et al. Effect of image quality on calcification 
     detection in digital mammography. Medical Physics. 2012;39(6):3202-3213.</p>  
<p>Chakraborty DP, Zhai X. On the meaning of the weighted alternative free-response 
     operating characteristic figure of merit. Medical physics. 2016;43(5):2548-2557.</p>
<p>Chakraborty DP. (2017) Observer Performance Methods for Diagnostic Imaging - 
     Foundations, Modeling, and Applications with R-Based Examples. 
     Taylor-Francis, LLC.</p>
    </div>

  </div>
  <div class="col-md-3 hidden-xs hidden-sm" id="pkgdown-sidebar">
    <nav id="toc" data-toggle="toc" class="sticky-top"><h2 data-toc-skip>Contents</h2>
    </nav></div>
</div>


      <footer><div class="copyright">
  <p></p><p>Developed by Dev Chakraborty, Xuetong Zhai.</p>
</div>

<div class="pkgdown">
  <p></p><p>Site built with <a href="https://pkgdown.r-lib.org/" class="external-link">pkgdown</a> 2.0.6.</p>
</div>

      </footer></div>

  


  

  </body></html>

